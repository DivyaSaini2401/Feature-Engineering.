{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Theoritical Questions\n",
        "**Q.1  What is a parameter?**\n",
        " - A parameter is a variable listed in a function’s definition that receives an argument when the function is called.\n",
        "\n",
        " def greet(name):\n",
        "   \n",
        "    print(\"Hello, \" + name)\n",
        "\n",
        "greet(\"Alice\")  # \"Alice\" is the argument passed to the parameter \"name\"\n",
        "\n",
        "\n",
        "**Q.2 What is correlation?What does negative correlation mean?**\n",
        "\n",
        "  - Correlation is a statistical measure that indicates the strength and direction of the relationship between two variables. It ranges from -1 to 1\n",
        "\n",
        "  A negative correlation means that when one variable increases, the other decreases. It is represented by a correlation coefficient between -1 and 0.\n",
        "\n",
        "**Q.3 Define Machine Learning. What are the main components in Machine Learning?**\n",
        "\n",
        "  - Machine Learning (ML) is a branch of artificial intelligence (AI) that enables computers to learn patterns from data and make predictions or decisions without being explicitly programmed\n",
        "\n",
        "  Main Components of Machine Learning\n",
        "\n",
        "1. Data – The foundation of ML, including training and testing datasets.\n",
        "\n",
        "2. Features – Relevant attributes or characteristics extracted from data for learning.\n",
        "\n",
        "\n",
        "3. Model – The mathematical or algorithmic framework used to learn from data.\n",
        "\n",
        "4. Algorithm – The method used to train the model (e.g., decision trees, neural networks).\n",
        "\n",
        "5. Loss Function – A measure of how well the model's predictions match the actual values.\n",
        "\n",
        "6. Training Process – The process of feeding data into the model to adjust parameters.\n",
        "\n",
        "7. Evaluation Metrics – Used to assess the model’s performance (e.g., accuracy, precision).\n",
        "\n",
        "8. Hyperparameters – Tunable settings that affect model performance (e.g., learning rate).\n",
        "\n",
        "9. Prediction/Inference – Using the trained model to make predictions on new data.\n",
        "\n",
        "**Q.4 How does loss value help in determining whether the model is good or not?**\n",
        "\n",
        " - The loss value is a key indicator of how well a machine learning model is performing. It measures the difference between the predicted output and the actual target values. A lower loss generally indicates a better model.\n",
        "\n",
        "**Q.5 What are continuous and categorical variables?**\n",
        "\n",
        " - Continuous Variables\n",
        "\n",
        " These are numerical variables that can take an infinite number of values within a range.\n",
        "\n",
        " They can be measured and have decimal precision\n",
        "\n",
        " - Categorical Variables\n",
        "\n",
        " These variables represent distinct groups or categories.\n",
        "\n",
        " They cannot be measured numerically but can be labeled or classified.\n",
        "\n",
        "**Q.7 How do we handle categorical variables in Machine Learning? What are the common technique?**\n",
        "\n",
        " - Categorical variables need to be converted into numerical values for ML models. Common techniques include:\n",
        "\n",
        "1. Label Encoding – Assigns unique integers to categories (best for ordinal data).\n",
        "\n",
        "2. One-Hot Encoding (OHE) – Creates binary columns for each category (best for nominal data).\n",
        "\n",
        "3. Ordinal Encoding – Assigns ordered integers to categories with a meaningful rank.\n",
        "\n",
        "4. Frequency Encoding – Replaces categories with their occurrence count.\n",
        "\n",
        "5. Target Encoding – Replaces categories with the mean of the target variable (risk of data leakage).\n",
        "\n",
        "6. Binary Encoding – Converts categories into binary form, reducing dimensionality.\n",
        "\n",
        "7.  Hash Encoding – Uses a hash function to map categories to a fixed number of features.\n",
        "\n",
        "**Q.7 What do you mean by training and testing a dataset?**\n",
        "\n",
        "  In machine learning, a dataset is divided into two main parts:\n",
        "\n",
        "  - Training Dataset\n",
        "\n",
        "Used to train the model by allowing it to learn patterns from the data.\n",
        "The model adjusts its parameters based on this dataset.\n",
        "Usually comprises 70-80% of the total data.\n",
        "\n",
        " - Testing Dataset\n",
        "\n",
        "Used to evaluate the model's performance on unseen data.\n",
        "Helps assess accuracy and generalization.\n",
        "Typically 20-30% of the total data.\n",
        "\n",
        "**Q.8 What is sklearn.preprocessing?**\n",
        "\n",
        " - sklearn.preprocessing in Scikit-Learn\n",
        "\n",
        " sklearn.preprocessing is a module in Scikit-Learn that provides various functions to transform and scale data before feeding it into a machine learning model.\n",
        "\n",
        "**Q.9 What is a Test set?**\n",
        "  \n",
        "  - A test set is a portion of the dataset used to evaluate the performance of a trained machine learning model.\n",
        "\n",
        "  Key Characteristics:\n",
        "\n",
        "   It is never used during training.\n",
        "\n",
        "   It helps assess how well the model generalizes to new, unseen data.\n",
        "\n",
        "  Typically makes up 20-30% of the total dataset.\n",
        "\n",
        "\n",
        "**Q.10 How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?**\n",
        "\n",
        " -In Scikit-Learn, we use the train_test_split() function from sklearn.model_selection to divide the dataset into training and testing sets.\n",
        "\n",
        "\n",
        "\n",
        "### **Approach to a Machine Learning Problem**  \n",
        "\n",
        "1️⃣ **Understand the Problem Statement**  \n",
        "   - Define the objective (e.g., classification, regression).  \n",
        "   - Identify key business or research questions.  \n",
        "\n",
        "2️⃣ **Collect and Explore Data**  \n",
        "   - Gather relevant datasets.  \n",
        "   - Perform **exploratory data analysis (EDA)** to understand distributions, missing values, and correlations.  \n",
        "\n",
        "3️⃣ **Preprocess Data**  \n",
        "   - Handle **missing values** (imputation, removal).  \n",
        "   - Convert **categorical variables** (label encoding, one-hot encoding).  \n",
        "   - Normalize/scale numerical features if needed.  \n",
        "\n",
        "4️⃣ **Split Data**  \n",
        "   - Divide data into **training** and **testing** sets.  \n",
        "   - Optionally, use a **validation set** for hyperparameter tuning.  \n",
        "\n",
        "5️⃣ **Select and Train Model**  \n",
        "   - Choose an appropriate **algorithm** (e.g., Decision Tree, SVM, Neural Networks).  \n",
        "   - Train the model on the **training set**.  \n",
        "\n",
        "6️⃣ **Evaluate Model Performance**  \n",
        "   - Use the **test set** to measure accuracy, precision, recall, F1-score, RMSE, etc.  \n",
        "   - Detect **overfitting** or **underfitting**.  \n",
        "\n",
        "7️⃣ **Hyperparameter Tuning & Optimization**  \n",
        "   - Improve performance using techniques like **GridSearchCV** or **RandomizedSearchCV**.  \n",
        "\n",
        "8️⃣ **Deploy the Model**  \n",
        "   - Save the trained model using `joblib` or `pickle`.  \n",
        "   - Deploy in production using APIs, cloud, or embedded systems.  \n",
        "\n",
        "9️⃣ **Monitor & Improve**  \n",
        "   - Continuously **track model performance** and update with new data.  \n",
        "\n",
        "**Q.11 Why do we have to perform EDA before fitting a model to the data?**\n",
        "\n",
        "\n",
        " EDA (Exploratory Data Analysis) helps ensure data quality and improves model performance by:  \n",
        "\n",
        "1️⃣ **Understanding Data Distribution** – Identifies patterns, trends, and skewness.  \n",
        "2️⃣ **Handling Missing Values** – Detects and fills or removes null values.  \n",
        "3️⃣ **Detecting Outliers** – Prevents skewed model training.  \n",
        "4️⃣ **Checking Feature Correlation** – Reduces redundancy and multicollinearity.  \n",
        "5️⃣ **Identifying Data Imbalance** – Avoids biased predictions.  \n",
        "6️⃣ **Guiding Feature Engineering** – Helps with encoding, scaling, and selection.  \n",
        "\n",
        "**Q.12 What is correlation?**\n",
        "\n",
        "  -  Correlation is a statistical measure that indicates the strength and direction of the relationship between two variables. It ranges from -1 to 1\n",
        "\n",
        "\n",
        "\n",
        "**Q.13 What does negative correlation mean?**\n",
        "  \n",
        "  -   A negative correlation means that when one variable increases, the other decreases. It is represented by a correlation coefficient between -1 and 0.\n",
        "\n",
        "\n",
        "**Q.14 How can you find correlation between variables in Python?**\n",
        "\n",
        "   - we can find the correlation between variables in Python using:  \n",
        "\n",
        "1. **Pandas** (`.corr()`) for Pearson, Spearman, or Kendall correlation:  \n",
        "   ```python\n",
        "   df.corr(method='pearson')  # Default is Pearson\n",
        "   ```\n",
        "\n",
        "2. **NumPy** (`np.corrcoef()`) for Pearson correlation:  \n",
        "   ```python\n",
        "   np.corrcoef(x, y)[0, 1]\n",
        "   ```\n",
        "\n",
        "3. **SciPy** (`pearsonr`, `spearmanr`, `kendalltau`) for correlation with p-values:  \n",
        "   ```python\n",
        "   from scipy.stats import pearsonr\n",
        "   pearsonr(x, y)\n",
        "   ```\n",
        "\n",
        "4. **Seaborn** (for visualization with a heatmap):  \n",
        "   ```python\n",
        "   sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "   ```\n",
        "\n",
        "**Q.15 What is causation? Explain difference between correlation and causation with an example**\n",
        "\n",
        "\n",
        "  - Causation (also called **causality**) means that one event **directly causes** another. In other words, a change in one variable **results in** a change in another.  \n",
        "\n",
        "🔹 **Example of Causation:**  \n",
        "If you increase the temperature of water, it starts boiling. Here, increasing temperature **directly causes** boiling.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Difference Between Correlation and Causation**  \n",
        "\n",
        "| Feature        | Correlation                         | Causation                           |\n",
        "|---------------|-----------------------------------|-----------------------------------|\n",
        "| **Definition** | A relationship where two variables move together (positively or negatively). | One variable directly causes a change in another. |\n",
        "| **Direction**  | No direct cause-effect relationship. | A direct cause-effect relationship exists. |\n",
        "| **Example**    | Ice cream sales and drowning rates are correlated (both increase in summer). | Eating contaminated food **causes** food poisoning. |\n",
        "\n",
        "---\n",
        "\n",
        "### **Example to Illustrate Difference**  \n",
        "\n",
        "📌 **Scenario:**  \n",
        "A study finds that people who exercise more tend to be happier.  \n",
        "\n",
        "- **Correlation:** Exercise and happiness are related, but exercise may not be the **cause**. Other factors like social interaction, better health, or endorphins could be involved.  \n",
        "- **Causation:** If a controlled experiment proves that increasing exercise levels **directly** leads to increased happiness, then we can say exercise **causes** happiness.  \n",
        "\n",
        "**Q.16 What is an Optimizer? What are different types of optimizers? Explain each with an example.**\n",
        "\n",
        "\n",
        "An **optimizer** is an algorithm that adjusts a machine learning model’s weights to **minimize the loss function** and improve accuracy.  \n",
        "\n",
        "---\n",
        "\n",
        "**Types of Optimizers in Machine Learning**  \n",
        "\n",
        "**1. Gradient Descent**  \n",
        "Updates weights based on the gradient of the loss function.  \n",
        "🔹 **Example (SGD in TensorFlow)**  \n",
        "```python\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "optimizer = SGD(learning_rate=0.01)\n",
        "```\n",
        "\n",
        " **2. Momentum-Based Optimizer (Momentum SGD)**  \n",
        "Uses momentum to speed up training and avoid local minima.  \n",
        "🔹 **Example:**  \n",
        "```python\n",
        "optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
        "```\n",
        " **3. AdaGrad (Adaptive Gradient Algorithm)**  \n",
        "Adapts learning rates based on past gradients, useful for sparse data.  \n",
        "🔹 **Example:**  \n",
        "```python\n",
        "from tensorflow.keras.optimizers import Adagrad\n",
        "optimizer = Adagrad(learning_rate=0.01)\n",
        "```\n",
        "\n",
        "**4. RMSprop (Root Mean Square Propagation)**  \n",
        "Maintains a moving average of squared gradients for better stability.  \n",
        "🔹 **Example:**  \n",
        "```python\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "optimizer = RMSprop(learning_rate=0.01)\n",
        "```\n",
        "\n",
        " **5. Adam (Adaptive Moment Estimation) [Most Common]**  \n",
        "Combines Momentum and RMSprop for adaptive learning rates.  \n",
        "🔹 **Example:**  \n",
        "```python\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "```\n",
        "\n",
        " **6. AdamW (Adam with Weight Decay)**  \n",
        "Prevents overfitting by adding weight decay.  \n",
        "🔹 **Example:**  \n",
        "```python\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "optimizer = AdamW(learning_rate=0.001, weight_decay=0.01)\n",
        "```\n",
        "**Q.17 What is sklearn.linear_model ?**\n",
        "\n",
        " - sklearn.linear_model is a module in scikit-learn that provides various linear models for regression and classification tasks. These models assume a linear relationship between input features and the target variable.\n",
        "\n",
        "\n",
        "**Q.18 What does model.fit() do? What arguments must be given?**\n",
        "\n",
        " - model.fit() is a method used in scikit-learn to train a machine learning model by learning patterns from the input data. It adjusts the model’s parameters (like weights in linear regression) based on the given dataset.\n",
        "\n",
        "\n",
        "**Q.19 What does model.predict() do? What arguments must be given?**\n",
        "\n",
        "\n",
        " - 'model.predict()` is used in **scikit-learn** to make predictions on new data after training a model.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Arguments for `model.predict()`**\n",
        "```python\n",
        "model.predict(X_new)\n",
        "```\n",
        "- **`X_new`** → New input data (must have the same number of features as training data).  \n",
        "\n",
        "---\n",
        "\n",
        "### **Example (Regression Prediction)**  \n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X_train = [[1], [2], [3], [4], [5]]\n",
        "y_train = [2, 4, 6, 8, 10]\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "X_new = [[6], [7]]  # New inputs\n",
        "y_pred = model.predict(X_new)\n",
        "\n",
        "print(y_pred)  # Output: [12. 14.]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Example (Classification Prediction)**  \n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X_train = [[1], [2], [3], [4], [5]]\n",
        "y_train = [0, 0, 1, 1, 1]\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "X_new = [[3.5]]\n",
        "print(model.predict(X_new))  # Output: [1]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Points**\n",
        "- Used for **making predictions** after training a model.  \n",
        "- Requires **`X_new`** (new input data).  \n",
        "- Returns **predicted values** for regression or **class labels** for classification.  \n",
        "- Use `predict_proba()` for class probabilities.  \n",
        "\n",
        "**Q.20 What are continuous and categorical variables?**\n",
        "\n",
        "Continuous Variables\n",
        "\n",
        "These are numerical variables that can take an infinite number of values within a range.\n",
        "\n",
        "They can be measured and have decimal precision\n",
        "\n",
        "Categorical Variables\n",
        "\n",
        "These variables represent distinct groups or categories.\n",
        "\n",
        "They cannot be measured numerically but can be labeled or classified.\n",
        "\n",
        "**Q.21What is feature scaling? How does it help in Machine Learning?**\n",
        "\n",
        " - Feature scaling is the process of normalizing or standardizing numerical features to a common scale without distorting differences in the data. It is essential in machine learning to improve model performance and training stability.\n",
        "\n",
        "it help in Machine learning by the following way\n",
        "Improves Model Performance – Many algorithms (e.g., Gradient Descent, SVM, KNN) perform better when features are on the same scale.\n",
        "\n",
        "Faster Convergence – Helps optimization algorithms (e.g., Gradient Descent) converge faster.\n",
        "\n",
        "Prevents Bias – Models like KNN and K-Means use distance calculations, so unscaled features can dominate the results.\n",
        "\n",
        "\n",
        "**Q22How do we perform scaling in Python?**\n",
        "\n",
        "\n",
        " - **Feature Scaling in Python**  \n",
        "\n",
        "We use **scikit-learn** to scale features using two common methods:  \n",
        "\n",
        " **1. Min-Max Scaling (Normalization)**  \n",
        "Scales data between **0 and 1**.  \n",
        "```python\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(X)  # X is your feature matrix\n",
        "```\n",
        " **2. Standardization (Z-score Scaling)**  \n",
        "Centers data around **mean = 0** and **std = 1**.  \n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(X)\n",
        "```\n",
        "\n",
        "**Q.23  What is sklearn.preprocessing?**\n",
        "\n",
        "sklearn.preprocessing in Scikit-Learn\n",
        "\n",
        "sklearn.preprocessing is a module in Scikit-Learn that provides various functions to transform and scale data before feeding it into a machine learning model.\n",
        "\n",
        "**Q.24How do we split data for model fitting (training and testing) in Python?**\n",
        "\n",
        " - -In Scikit-Learn, we use the train_test_split() function from sklearn.model_selection to divide the dataset into training and testing sets.\n",
        "\n",
        "\n",
        "**Q.25\n"
      ],
      "metadata": {
        "id": "CotZ1u7UPIT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Theoritical Questions\n",
        "**Q.1  What is a parameter?**\n",
        " - A parameter is a variable listed in a function’s definition that receives an argument when the function is called.\n",
        "\n",
        " def greet(name):\n",
        "   \n",
        "    print(\"Hello, \" + name)\n",
        "\n",
        "greet(\"Alice\")  # \"Alice\" is the argument passed to the parameter \"name\"\n",
        "\n",
        "\n",
        "**Q.2 What is correlation?What does negative correlation mean?**\n",
        "\n",
        "  - Correlation is a statistical measure that indicates the strength and direction of the relationship between two variables. It ranges from -1 to 1\n",
        "\n",
        "  A negative correlation means that when one variable increases, the other decreases. It is represented by a correlation coefficient between -1 and 0.\n",
        "\n",
        "**Q.3 Define Machine Learning. What are the main components in Machine Learning?**\n",
        "\n",
        "  - Machine Learning (ML) is a branch of artificial intelligence (AI) that enables computers to learn patterns from data and make predictions or decisions without being explicitly programmed\n",
        "\n",
        "  Main Components of Machine Learning\n",
        "\n",
        "1. Data – The foundation of ML, including training and testing datasets.\n",
        "\n",
        "2. Features – Relevant attributes or characteristics extracted from data for learning.\n",
        "\n",
        "\n",
        "3. Model – The mathematical or algorithmic framework used to learn from data.\n",
        "\n",
        "4. Algorithm – The method used to train the model (e.g., decision trees, neural networks).\n",
        "\n",
        "5. Loss Function – A measure of how well the model's predictions match the actual values.\n",
        "\n",
        "6. Training Process – The process of feeding data into the model to adjust parameters.\n",
        "\n",
        "7. Evaluation Metrics – Used to assess the model’s performance (e.g., accuracy, precision).\n",
        "\n",
        "8. Hyperparameters – Tunable settings that affect model performance (e.g., learning rate).\n",
        "\n",
        "9. Prediction/Inference – Using the trained model to make predictions on new data.\n",
        "\n",
        "**Q.4 How does loss value help in determining whether the model is good or not?**\n",
        "\n",
        " - The loss value is a key indicator of how well a machine learning model is performing. It measures the difference between the predicted output and the actual target values. A lower loss generally indicates a better model.\n",
        "\n",
        "**Q.5 What are continuous and categorical variables?**\n",
        "\n",
        " - Continuous Variables\n",
        "\n",
        " These are numerical variables that can take an infinite number of values within a range.\n",
        "\n",
        " They can be measured and have decimal precision\n",
        "\n",
        " - Categorical Variables\n",
        "\n",
        " These variables represent distinct groups or categories.\n",
        "\n",
        " They cannot be measured numerically but can be labeled or classified.\n",
        "\n",
        "**Q.7 How do we handle categorical variables in Machine Learning? What are the common technique?**\n",
        "\n",
        " - Categorical variables need to be converted into numerical values for ML models. Common techniques include:\n",
        "\n",
        "1. Label Encoding – Assigns unique integers to categories (best for ordinal data).\n",
        "\n",
        "2. One-Hot Encoding (OHE) – Creates binary columns for each category (best for nominal data).\n",
        "\n",
        "3. Ordinal Encoding – Assigns ordered integers to categories with a meaningful rank.\n",
        "\n",
        "4. Frequency Encoding – Replaces categories with their occurrence count.\n",
        "\n",
        "5. Target Encoding – Replaces categories with the mean of the target variable (risk of data leakage).\n",
        "\n",
        "6. Binary Encoding – Converts categories into binary form, reducing dimensionality.\n",
        "\n",
        "7.  Hash Encoding – Uses a hash function to map categories to a fixed number of features.\n",
        "\n",
        "**Q.7 What do you mean by training and testing a dataset?**\n",
        "\n",
        "  In machine learning, a dataset is divided into two main parts:\n",
        "\n",
        "  - Training Dataset\n",
        "\n",
        "Used to train the model by allowing it to learn patterns from the data.\n",
        "The model adjusts its parameters based on this dataset.\n",
        "Usually comprises 70-80% of the total data.\n",
        "\n",
        " - Testing Dataset\n",
        "\n",
        "Used to evaluate the model's performance on unseen data.\n",
        "Helps assess accuracy and generalization.\n",
        "Typically 20-30% of the total data.\n",
        "\n",
        "**Q.8 What is sklearn.preprocessing?**\n",
        "\n",
        " - sklearn.preprocessing in Scikit-Learn\n",
        "\n",
        " sklearn.preprocessing is a module in Scikit-Learn that provides various functions to transform and scale data before feeding it into a machine learning model.\n",
        "\n",
        "**Q.9 What is a Test set?**\n",
        "  \n",
        "  - A test set is a portion of the dataset used to evaluate the performance of a trained machine learning model.\n",
        "\n",
        "  Key Characteristics:\n",
        "\n",
        "   It is never used during training.\n",
        "\n",
        "   It helps assess how well the model generalizes to new, unseen data.\n",
        "\n",
        "  Typically makes up 20-30% of the total dataset.\n",
        "\n",
        "\n",
        "**Q.10 How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?**\n",
        "\n",
        " -In Scikit-Learn, we use the train_test_split() function from sklearn.model_selection to divide the dataset into training and testing sets.\n",
        "\n",
        "\n",
        "\n",
        "### **Approach to a Machine Learning Problem**  \n",
        "\n",
        "1️⃣ **Understand the Problem Statement**  \n",
        "   - Define the objective (e.g., classification, regression).  \n",
        "   - Identify key business or research questions.  \n",
        "\n",
        "2️⃣ **Collect and Explore Data**  \n",
        "   - Gather relevant datasets.  \n",
        "   - Perform **exploratory data analysis (EDA)** to understand distributions, missing values, and correlations.  \n",
        "\n",
        "3️⃣ **Preprocess Data**  \n",
        "   - Handle **missing values** (imputation, removal).  \n",
        "   - Convert **categorical variables** (label encoding, one-hot encoding).  \n",
        "   - Normalize/scale numerical features if needed.  \n",
        "\n",
        "4️⃣ **Split Data**  \n",
        "   - Divide data into **training** and **testing** sets.  \n",
        "   - Optionally, use a **validation set** for hyperparameter tuning.  \n",
        "\n",
        "5️⃣ **Select and Train Model**  \n",
        "   - Choose an appropriate **algorithm** (e.g., Decision Tree, SVM, Neural Networks).  \n",
        "   - Train the model on the **training set**.  \n",
        "\n",
        "6️⃣ **Evaluate Model Performance**  \n",
        "   - Use the **test set** to measure accuracy, precision, recall, F1-score, RMSE, etc.  \n",
        "   - Detect **overfitting** or **underfitting**.  \n",
        "\n",
        "7️⃣ **Hyperparameter Tuning & Optimization**  \n",
        "   - Improve performance using techniques like **GridSearchCV** or **RandomizedSearchCV**.  \n",
        "\n",
        "8️⃣ **Deploy the Model**  \n",
        "   - Save the trained model using `joblib` or `pickle`.  \n",
        "   - Deploy in production using APIs, cloud, or embedded systems.  \n",
        "\n",
        "9️⃣ **Monitor & Improve**  \n",
        "   - Continuously **track model performance** and update with new data.  \n",
        "\n",
        "**Q.11 Why do we have to perform EDA before fitting a model to the data?**\n",
        "\n",
        "\n",
        " EDA (Exploratory Data Analysis) helps ensure data quality and improves model performance by:  \n",
        "\n",
        "1️⃣ **Understanding Data Distribution** – Identifies patterns, trends, and skewness.  \n",
        "2️⃣ **Handling Missing Values** – Detects and fills or removes null values.  \n",
        "3️⃣ **Detecting Outliers** – Prevents skewed model training.  \n",
        "4️⃣ **Checking Feature Correlation** – Reduces redundancy and multicollinearity.  \n",
        "5️⃣ **Identifying Data Imbalance** – Avoids biased predictions.  \n",
        "6️⃣ **Guiding Feature Engineering** – Helps with encoding, scaling, and selection.  \n",
        "\n",
        "**Q.12 What is correlation?**\n",
        "\n",
        "  -  Correlation is a statistical measure that indicates the strength and direction of the relationship between two variables. It ranges from -1 to 1\n",
        "\n",
        "\n",
        "\n",
        "**Q.13 What does negative correlation mean?**\n",
        "  \n",
        "  -   A negative correlation means that when one variable increases, the other decreases. It is represented by a correlation coefficient between -1 and 0.\n",
        "\n",
        "\n",
        "**Q.14 How can you find correlation between variables in Python?**\n",
        "\n",
        "   - we can find the correlation between variables in Python using:  \n",
        "\n",
        "1. **Pandas** (`.corr()`) for Pearson, Spearman, or Kendall correlation:  \n",
        "   ```python\n",
        "   df.corr(method='pearson')  # Default is Pearson\n",
        "   ```\n",
        "\n",
        "2. **NumPy** (`np.corrcoef()`) for Pearson correlation:  \n",
        "   ```python\n",
        "   np.corrcoef(x, y)[0, 1]\n",
        "   ```\n",
        "\n",
        "3. **SciPy** (`pearsonr`, `spearmanr`, `kendalltau`) for correlation with p-values:  \n",
        "   ```python\n",
        "   from scipy.stats import pearsonr\n",
        "   pearsonr(x, y)\n",
        "   ```\n",
        "\n",
        "4. **Seaborn** (for visualization with a heatmap):  \n",
        "   ```python\n",
        "   sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "   ```\n",
        "\n",
        "**Q.15 What is causation? Explain difference between correlation and causation with an example**\n",
        "\n",
        "\n",
        "  - Causation (also called **causality**) means that one event **directly causes** another. In other words, a change in one variable **results in** a change in another.  \n",
        "\n",
        "🔹 **Example of Causation:**  \n",
        "If you increase the temperature of water, it starts boiling. Here, increasing temperature **directly causes** boiling.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Difference Between Correlation and Causation**  \n",
        "\n",
        "| Feature        | Correlation                         | Causation                           |\n",
        "|---------------|-----------------------------------|-----------------------------------|\n",
        "| **Definition** | A relationship where two variables move together (positively or negatively). | One variable directly causes a change in another. |\n",
        "| **Direction**  | No direct cause-effect relationship. | A direct cause-effect relationship exists. |\n",
        "| **Example**    | Ice cream sales and drowning rates are correlated (both increase in summer). | Eating contaminated food **causes** food poisoning. |\n",
        "\n",
        "---\n",
        "\n",
        "### **Example to Illustrate Difference**  \n",
        "\n",
        "📌 **Scenario:**  \n",
        "A study finds that people who exercise more tend to be happier.  \n",
        "\n",
        "- **Correlation:** Exercise and happiness are related, but exercise may not be the **cause**. Other factors like social interaction, better health, or endorphins could be involved.  \n",
        "- **Causation:** If a controlled experiment proves that increasing exercise levels **directly** leads to increased happiness, then we can say exercise **causes** happiness.  \n",
        "\n",
        "**Q.16 What is an Optimizer? What are different types of optimizers? Explain each with an example.**\n",
        "\n",
        "\n",
        "An **optimizer** is an algorithm that adjusts a machine learning model’s weights to **minimize the loss function** and improve accuracy.  \n",
        "\n",
        "---\n",
        "\n",
        "**Types of Optimizers in Machine Learning**  \n",
        "\n",
        "**1. Gradient Descent**  \n",
        "Updates weights based on the gradient of the loss function.  \n",
        "🔹 **Example (SGD in TensorFlow)**  \n",
        "```python\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "optimizer = SGD(learning_rate=0.01)\n",
        "```\n",
        "\n",
        " **2. Momentum-Based Optimizer (Momentum SGD)**  \n",
        "Uses momentum to speed up training and avoid local minima.  \n",
        "🔹 **Example:**  \n",
        "```python\n",
        "optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
        "```\n",
        " **3. AdaGrad (Adaptive Gradient Algorithm)**  \n",
        "Adapts learning rates based on past gradients, useful for sparse data.  \n",
        "🔹 **Example:**  \n",
        "```python\n",
        "from tensorflow.keras.optimizers import Adagrad\n",
        "optimizer = Adagrad(learning_rate=0.01)\n",
        "```\n",
        "\n",
        "**4. RMSprop (Root Mean Square Propagation)**  \n",
        "Maintains a moving average of squared gradients for better stability.  \n",
        "🔹 **Example:**  \n",
        "```python\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "optimizer = RMSprop(learning_rate=0.01)\n",
        "```\n",
        "\n",
        " **5. Adam (Adaptive Moment Estimation) [Most Common]**  \n",
        "Combines Momentum and RMSprop for adaptive learning rates.  \n",
        "🔹 **Example:**  \n",
        "```python\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "```\n",
        "\n",
        " **6. AdamW (Adam with Weight Decay)**  \n",
        "Prevents overfitting by adding weight decay.  \n",
        "🔹 **Example:**  \n",
        "```python\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "optimizer = AdamW(learning_rate=0.001, weight_decay=0.01)\n",
        "```\n",
        "**Q.17 What is sklearn.linear_model ?**\n",
        "\n",
        " - sklearn.linear_model is a module in scikit-learn that provides various linear models for regression and classification tasks. These models assume a linear relationship between input features and the target variable.\n",
        "\n",
        "\n",
        "**Q.18 What does model.fit() do? What arguments must be given?**\n",
        "\n",
        " - model.fit() is a method used in scikit-learn to train a machine learning model by learning patterns from the input data. It adjusts the model’s parameters (like weights in linear regression) based on the given dataset.\n",
        "\n",
        "\n",
        "**Q.19 What does model.predict() do? What arguments must be given?**\n",
        "\n",
        "\n",
        " - 'model.predict()` is used in **scikit-learn** to make predictions on new data after training a model.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Arguments for `model.predict()`**\n",
        "```python\n",
        "model.predict(X_new)\n",
        "```\n",
        "- **`X_new`** → New input data (must have the same number of features as training data).  \n",
        "\n",
        "---\n",
        "\n",
        "### **Example (Regression Prediction)**  \n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X_train = [[1], [2], [3], [4], [5]]\n",
        "y_train = [2, 4, 6, 8, 10]\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "X_new = [[6], [7]]  # New inputs\n",
        "y_pred = model.predict(X_new)\n",
        "\n",
        "print(y_pred)  # Output: [12. 14.]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Example (Classification Prediction)**  \n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X_train = [[1], [2], [3], [4], [5]]\n",
        "y_train = [0, 0, 1, 1, 1]\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "X_new = [[3.5]]\n",
        "print(model.predict(X_new))  # Output: [1]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Points**\n",
        "- Used for **making predictions** after training a model.  \n",
        "- Requires **`X_new`** (new input data).  \n",
        "- Returns **predicted values** for regression or **class labels** for classification.  \n",
        "- Use `predict_proba()` for class probabilities.  \n",
        "\n",
        "**Q.20 What are continuous and categorical variables?**\n",
        "\n",
        "Continuous Variables\n",
        "\n",
        "These are numerical variables that can take an infinite number of values within a range.\n",
        "\n",
        "They can be measured and have decimal precision\n",
        "\n",
        "Categorical Variables\n",
        "\n",
        "These variables represent distinct groups or categories.\n",
        "\n",
        "They cannot be measured numerically but can be labeled or classified.\n",
        "\n",
        "**Q.21What is feature scaling? How does it help in Machine Learning?**\n",
        "\n",
        " - Feature scaling is the process of normalizing or standardizing numerical features to a common scale without distorting differences in the data. It is essential in machine learning to improve model performance and training stability.\n",
        "\n",
        "it help in Machine learning by the following way\n",
        "Improves Model Performance – Many algorithms (e.g., Gradient Descent, SVM, KNN) perform better when features are on the same scale.\n",
        "\n",
        "Faster Convergence – Helps optimization algorithms (e.g., Gradient Descent) converge faster.\n",
        "\n",
        "Prevents Bias – Models like KNN and K-Means use distance calculations, so unscaled features can dominate the results.\n",
        "\n",
        "\n",
        "**Q22How do we perform scaling in Python?**\n",
        "\n",
        "\n",
        " - **Feature Scaling in Python**  \n",
        "\n",
        "We use **scikit-learn** to scale features using two common methods:  \n",
        "\n",
        " **1. Min-Max Scaling (Normalization)**  \n",
        "Scales data between **0 and 1**.  \n",
        "```python\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(X)  # X is your feature matrix\n",
        "```\n",
        " **2. Standardization (Z-score Scaling)**  \n",
        "Centers data around **mean = 0** and **std = 1**.  \n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(X)\n",
        "```\n",
        "\n",
        "**Q.23  What is sklearn.preprocessing?**\n",
        "\n",
        "sklearn.preprocessing in Scikit-Learn\n",
        "\n",
        "sklearn.preprocessing is a module in Scikit-Learn that provides various functions to transform and scale data before feeding it into a machine learning model.\n",
        "\n",
        "**Q.24How do we split data for model fitting (training and testing) in Python?**\n",
        "\n",
        " - -In Scikit-Learn, we use the train_test_split() function from sklearn.model_selection to divide the dataset into training and testing sets.\n",
        "\n",
        "\n",
        "**Q.25\n"
      ],
      "metadata": {
        "id": "ODa_RC54nh5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Theoritical Questions\n",
        "**Q.1  What is a parameter?**\n",
        " - A parameter is a variable listed in a function’s definition that receives an argument when the function is called.\n",
        "\n",
        " def greet(name):\n",
        "   \n",
        "    print(\"Hello, \" + name)\n",
        "\n",
        "greet(\"Alice\")  # \"Alice\" is the argument passed to the parameter \"name\"\n",
        "\n",
        "\n",
        "**Q.2 What is correlation?What does negative correlation mean?**\n",
        "\n",
        "  - Correlation is a statistical measure that indicates the strength and direction of the relationship between two variables. It ranges from -1 to 1\n",
        "\n",
        "  A negative correlation means that when one variable increases, the other decreases. It is represented by a correlation coefficient between -1 and 0.\n",
        "\n",
        "**Q.3 Define Machine Learning. What are the main components in Machine Learning?**\n",
        "\n",
        "  - Machine Learning (ML) is a branch of artificial intelligence (AI) that enables computers to learn patterns from data and make predictions or decisions without being explicitly programmed\n",
        "\n",
        "  Main Components of Machine Learning\n",
        "\n",
        "1. Data – The foundation of ML, including training and testing datasets.\n",
        "\n",
        "2. Features – Relevant attributes or characteristics extracted from data for learning.\n",
        "\n",
        "\n",
        "3. Model – The mathematical or algorithmic framework used to learn from data.\n",
        "\n",
        "4. Algorithm – The method used to train the model (e.g., decision trees, neural networks).\n",
        "\n",
        "5. Loss Function – A measure of how well the model's predictions match the actual values.\n",
        "\n",
        "6. Training Process – The process of feeding data into the model to adjust parameters.\n",
        "\n",
        "7. Evaluation Metrics – Used to assess the model’s performance (e.g., accuracy, precision).\n",
        "\n",
        "8. Hyperparameters – Tunable settings that affect model performance (e.g., learning rate).\n",
        "\n",
        "9. Prediction/Inference – Using the trained model to make predictions on new data.\n",
        "\n",
        "**Q.4 How does loss value help in determining whether the model is good or not?**\n",
        "\n",
        " - The loss value is a key indicator of how well a machine learning model is performing. It measures the difference between the predicted output and the actual target values. A lower loss generally indicates a better model.\n",
        "\n",
        "**Q.5 What are continuous and categorical variables?**\n",
        "\n",
        " - Continuous Variables\n",
        "\n",
        " These are numerical variables that can take an infinite number of values within a range.\n",
        "\n",
        " They can be measured and have decimal precision\n",
        "\n",
        " - Categorical Variables\n",
        "\n",
        " These variables represent distinct groups or categories.\n",
        "\n",
        " They cannot be measured numerically but can be labeled or classified.\n",
        "\n",
        "**Q.7 How do we handle categorical variables in Machine Learning? What are the common technique?**\n",
        "\n",
        " - Categorical variables need to be converted into numerical values for ML models. Common techniques include:\n",
        "\n",
        "1. Label Encoding – Assigns unique integers to categories (best for ordinal data).\n",
        "\n",
        "2. One-Hot Encoding (OHE) – Creates binary columns for each category (best for nominal data).\n",
        "\n",
        "3. Ordinal Encoding – Assigns ordered integers to categories with a meaningful rank.\n",
        "\n",
        "4. Frequency Encoding – Replaces categories with their occurrence count.\n",
        "\n",
        "5. Target Encoding – Replaces categories with the mean of the target variable (risk of data leakage).\n",
        "\n",
        "6. Binary Encoding – Converts categories into binary form, reducing dimensionality.\n",
        "\n",
        "7.  Hash Encoding – Uses a hash function to map categories to a fixed number of features.\n",
        "\n",
        "**Q.7 What do you mean by training and testing a dataset?**\n",
        "\n",
        "  In machine learning, a dataset is divided into two main parts:\n",
        "\n",
        "  - Training Dataset\n",
        "\n",
        "Used to train the model by allowing it to learn patterns from the data.\n",
        "The model adjusts its parameters based on this dataset.\n",
        "Usually comprises 70-80% of the total data.\n",
        "\n",
        " - Testing Dataset\n",
        "\n",
        "Used to evaluate the model's performance on unseen data.\n",
        "Helps assess accuracy and generalization.\n",
        "Typically 20-30% of the total data.\n",
        "\n",
        "**Q.8 What is sklearn.preprocessing?**\n",
        "\n",
        " - sklearn.preprocessing in Scikit-Learn\n",
        "\n",
        " sklearn.preprocessing is a module in Scikit-Learn that provides various functions to transform and scale data before feeding it into a machine learning model.\n",
        "\n",
        "**Q.9 What is a Test set?**\n",
        "  \n",
        "  - A test set is a portion of the dataset used to evaluate the performance of a trained machine learning model.\n",
        "\n",
        "  Key Characteristics:\n",
        "\n",
        "   It is never used during training.\n",
        "\n",
        "   It helps assess how well the model generalizes to new, unseen data.\n",
        "\n",
        "  Typically makes up 20-30% of the total dataset.\n",
        "\n",
        "\n",
        "**Q.10 How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?**\n",
        "\n",
        " -In Scikit-Learn, we use the train_test_split() function from sklearn.model_selection to divide the dataset into training and testing sets.\n",
        "\n",
        "\n",
        "\n",
        "### **Approach to a Machine Learning Problem**  \n",
        "\n",
        "1️⃣ **Understand the Problem Statement**  \n",
        "   - Define the objective (e.g., classification, regression).  \n",
        "   - Identify key business or research questions.  \n",
        "\n",
        "2️⃣ **Collect and Explore Data**  \n",
        "   - Gather relevant datasets.  \n",
        "   - Perform **exploratory data analysis (EDA)** to understand distributions, missing values, and correlations.  \n",
        "\n",
        "3️⃣ **Preprocess Data**  \n",
        "   - Handle **missing values** (imputation, removal).  \n",
        "   - Convert **categorical variables** (label encoding, one-hot encoding).  \n",
        "   - Normalize/scale numerical features if needed.  \n",
        "\n",
        "4️⃣ **Split Data**  \n",
        "   - Divide data into **training** and **testing** sets.  \n",
        "   - Optionally, use a **validation set** for hyperparameter tuning.  \n",
        "\n",
        "5️⃣ **Select and Train Model**  \n",
        "   - Choose an appropriate **algorithm** (e.g., Decision Tree, SVM, Neural Networks).  \n",
        "   - Train the model on the **training set**.  \n",
        "\n",
        "6️⃣ **Evaluate Model Performance**  \n",
        "   - Use the **test set** to measure accuracy, precision, recall, F1-score, RMSE, etc.  \n",
        "   - Detect **overfitting** or **underfitting**.  \n",
        "\n",
        "7️⃣ **Hyperparameter Tuning & Optimization**  \n",
        "   - Improve performance using techniques like **GridSearchCV** or **RandomizedSearchCV**.  \n",
        "\n",
        "8️⃣ **Deploy the Model**  \n",
        "   - Save the trained model using `joblib` or `pickle`.  \n",
        "   - Deploy in production using APIs, cloud, or embedded systems.  \n",
        "\n",
        "9️⃣ **Monitor & Improve**  \n",
        "   - Continuously **track model performance** and update with new data.  \n",
        "\n",
        "**Q.11 Why do we have to perform EDA before fitting a model to the data?**\n",
        "\n",
        "\n",
        " EDA (Exploratory Data Analysis) helps ensure data quality and improves model performance by:  \n",
        "\n",
        "1️⃣ **Understanding Data Distribution** – Identifies patterns, trends, and skewness.  \n",
        "2️⃣ **Handling Missing Values** – Detects and fills or removes null values.  \n",
        "3️⃣ **Detecting Outliers** – Prevents skewed model training.  \n",
        "4️⃣ **Checking Feature Correlation** – Reduces redundancy and multicollinearity.  \n",
        "5️⃣ **Identifying Data Imbalance** – Avoids biased predictions.  \n",
        "6️⃣ **Guiding Feature Engineering** – Helps with encoding, scaling, and selection.  \n",
        "\n",
        "**Q.12 What is correlation?**\n",
        "\n",
        "  -  Correlation is a statistical measure that indicates the strength and direction of the relationship between two variables. It ranges from -1 to 1\n",
        "\n",
        "\n",
        "\n",
        "**Q.13 What does negative correlation mean?**\n",
        "  \n",
        "  -   A negative correlation means that when one variable increases, the other decreases. It is represented by a correlation coefficient between -1 and 0.\n",
        "\n",
        "\n",
        "**Q.14 How can you find correlation between variables in Python?**\n",
        "\n",
        "   - we can find the correlation between variables in Python using:  \n",
        "\n",
        "1. **Pandas** (`.corr()`) for Pearson, Spearman, or Kendall correlation:  \n",
        "   ```python\n",
        "   df.corr(method='pearson')  # Default is Pearson\n",
        "   ```\n",
        "\n",
        "2. **NumPy** (`np.corrcoef()`) for Pearson correlation:  \n",
        "   ```python\n",
        "   np.corrcoef(x, y)[0, 1]\n",
        "   ```\n",
        "\n",
        "3. **SciPy** (`pearsonr`, `spearmanr`, `kendalltau`) for correlation with p-values:  \n",
        "   ```python\n",
        "   from scipy.stats import pearsonr\n",
        "   pearsonr(x, y)\n",
        "   ```\n",
        "\n",
        "4. **Seaborn** (for visualization with a heatmap):  \n",
        "   ```python\n",
        "   sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "   ```\n",
        "\n",
        "**Q.15 What is causation? Explain difference between correlation and causation with an example**\n",
        "\n",
        "\n",
        "  - Causation (also called **causality**) means that one event **directly causes** another. In other words, a change in one variable **results in** a change in another.  \n",
        "\n",
        "🔹 **Example of Causation:**  \n",
        "If you increase the temperature of water, it starts boiling. Here, increasing temperature **directly causes** boiling.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Difference Between Correlation and Causation**  \n",
        "\n",
        "| Feature        | Correlation                         | Causation                           |\n",
        "|---------------|-----------------------------------|-----------------------------------|\n",
        "| **Definition** | A relationship where two variables move together (positively or negatively). | One variable directly causes a change in another. |\n",
        "| **Direction**  | No direct cause-effect relationship. | A direct cause-effect relationship exists. |\n",
        "| **Example**    | Ice cream sales and drowning rates are correlated (both increase in summer). | Eating contaminated food **causes** food poisoning. |\n",
        "\n",
        "---\n",
        "\n",
        "### **Example to Illustrate Difference**  \n",
        "\n",
        "📌 **Scenario:**  \n",
        "A study finds that people who exercise more tend to be happier.  \n",
        "\n",
        "- **Correlation:** Exercise and happiness are related, but exercise may not be the **cause**. Other factors like social interaction, better health, or endorphins could be involved.  \n",
        "- **Causation:** If a controlled experiment proves that increasing exercise levels **directly** leads to increased happiness, then we can say exercise **causes** happiness.  \n",
        "\n",
        "**Q.16 What is an Optimizer? What are different types of optimizers? Explain each with an example.**\n",
        "\n",
        "\n",
        "An **optimizer** is an algorithm that adjusts a machine learning model’s weights to **minimize the loss function** and improve accuracy.  \n",
        "\n",
        "---\n",
        "\n",
        "**Types of Optimizers in Machine Learning**  \n",
        "\n",
        "**1. Gradient Descent**  \n",
        "Updates weights based on the gradient of the loss function.  \n",
        "🔹 **Example (SGD in TensorFlow)**  \n",
        "```python\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "optimizer = SGD(learning_rate=0.01)\n",
        "```\n",
        "\n",
        " **2. Momentum-Based Optimizer (Momentum SGD)**  \n",
        "Uses momentum to speed up training and avoid local minima.  \n",
        "🔹 **Example:**  \n",
        "```python\n",
        "optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
        "```\n",
        " **3. AdaGrad (Adaptive Gradient Algorithm)**  \n",
        "Adapts learning rates based on past gradients, useful for sparse data.  \n",
        "🔹 **Example:**  \n",
        "```python\n",
        "from tensorflow.keras.optimizers import Adagrad\n",
        "optimizer = Adagrad(learning_rate=0.01)\n",
        "```\n",
        "\n",
        "**4. RMSprop (Root Mean Square Propagation)**  \n",
        "Maintains a moving average of squared gradients for better stability.  \n",
        "🔹 **Example:**  \n",
        "```python\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "optimizer = RMSprop(learning_rate=0.01)\n",
        "```\n",
        "\n",
        " **5. Adam (Adaptive Moment Estimation) [Most Common]**  \n",
        "Combines Momentum and RMSprop for adaptive learning rates.  \n",
        "🔹 **Example:**  \n",
        "```python\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "```\n",
        "\n",
        " **6. AdamW (Adam with Weight Decay)**  \n",
        "Prevents overfitting by adding weight decay.  \n",
        "🔹 **Example:**  \n",
        "```python\n",
        "from tensorflow.keras.optimizers import AdamW\n",
        "optimizer = AdamW(learning_rate=0.001, weight_decay=0.01)\n",
        "```\n",
        "**Q.17 What is sklearn.linear_model ?**\n",
        "\n",
        " - sklearn.linear_model is a module in scikit-learn that provides various linear models for regression and classification tasks. These models assume a linear relationship between input features and the target variable.\n",
        "\n",
        "\n",
        "**Q.18 What does model.fit() do? What arguments must be given?**\n",
        "\n",
        " - model.fit() is a method used in scikit-learn to train a machine learning model by learning patterns from the input data. It adjusts the model’s parameters (like weights in linear regression) based on the given dataset.\n",
        "\n",
        "\n",
        "**Q.19 What does model.predict() do? What arguments must be given?**\n",
        "\n",
        "\n",
        " - 'model.predict()` is used in **scikit-learn** to make predictions on new data after training a model.  \n",
        "\n",
        "---\n",
        "\n",
        "### **Arguments for `model.predict()`**\n",
        "```python\n",
        "model.predict(X_new)\n",
        "```\n",
        "- **`X_new`** → New input data (must have the same number of features as training data).  \n",
        "\n",
        "---\n",
        "\n",
        "### **Example (Regression Prediction)**  \n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X_train = [[1], [2], [3], [4], [5]]\n",
        "y_train = [2, 4, 6, 8, 10]\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "X_new = [[6], [7]]  # New inputs\n",
        "y_pred = model.predict(X_new)\n",
        "\n",
        "print(y_pred)  # Output: [12. 14.]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Example (Classification Prediction)**  \n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X_train = [[1], [2], [3], [4], [5]]\n",
        "y_train = [0, 0, 1, 1, 1]\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "X_new = [[3.5]]\n",
        "print(model.predict(X_new))  # Output: [1]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Points**\n",
        "- Used for **making predictions** after training a model.  \n",
        "- Requires **`X_new`** (new input data).  \n",
        "- Returns **predicted values** for regression or **class labels** for classification.  \n",
        "- Use `predict_proba()` for class probabilities.  \n",
        "\n",
        "**Q.20 What are continuous and categorical variables?**\n",
        "\n",
        "Continuous Variables\n",
        "\n",
        "These are numerical variables that can take an infinite number of values within a range.\n",
        "\n",
        "They can be measured and have decimal precision\n",
        "\n",
        "Categorical Variables\n",
        "\n",
        "These variables represent distinct groups or categories.\n",
        "\n",
        "They cannot be measured numerically but can be labeled or classified.\n",
        "\n",
        "**Q.21What is feature scaling? How does it help in Machine Learning?**\n",
        "\n",
        " - Feature scaling is the process of normalizing or standardizing numerical features to a common scale without distorting differences in the data. It is essential in machine learning to improve model performance and training stability.\n",
        "\n",
        "it help in Machine learning by the following way\n",
        "Improves Model Performance – Many algorithms (e.g., Gradient Descent, SVM, KNN) perform better when features are on the same scale.\n",
        "\n",
        "Faster Convergence – Helps optimization algorithms (e.g., Gradient Descent) converge faster.\n",
        "\n",
        "Prevents Bias – Models like KNN and K-Means use distance calculations, so unscaled features can dominate the results.\n",
        "\n",
        "\n",
        "**Q22How do we perform scaling in Python?**\n",
        "\n",
        "\n",
        " - **Feature Scaling in Python**  \n",
        "\n",
        "We use **scikit-learn** to scale features using two common methods:  \n",
        "\n",
        " **1. Min-Max Scaling (Normalization)**  \n",
        "Scales data between **0 and 1**.  \n",
        "```python\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(X)  # X is your feature matrix\n",
        "```\n",
        " **2. Standardization (Z-score Scaling)**  \n",
        "Centers data around **mean = 0** and **std = 1**.  \n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(X)\n",
        "```\n",
        "\n",
        "**Q.23  What is sklearn.preprocessing?**\n",
        "\n",
        "sklearn.preprocessing in Scikit-Learn\n",
        "\n",
        "sklearn.preprocessing is a module in Scikit-Learn that provides various functions to transform and scale data before feeding it into a machine learning model.\n",
        "\n",
        "**Q.24How do we split data for model fitting (training and testing) in Python?**\n",
        "\n",
        " - -In Scikit-Learn, we use the train_test_split() function from sklearn.model_selection to divide the dataset into training and testing sets.\n",
        "\n",
        "\n",
        "**Q.25 Explain data encoding ?**\n",
        "  \n",
        "\n",
        "  - Data encoding is the process of converting categorical data into a numerical format that machine learning models can understand. Since most ML models work with numbers, categorical data (like \"Red\", \"Blue\", \"Green\") must be encoded into numeric values.\n",
        "\n"
      ],
      "metadata": {
        "id": "nSydXgIYnl7t"
      }
    }
  ]
}